
\addcontentsline{toc}{chapter}{Analiza istniejącej literatury oraz dotychczasowych badań}
\chapter{Analiza istniejącej literatury oraz dotychczasowych badań}

\section{Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?}
% https://arxiv.org/pdf/2112.02125v1.pdf

\subsection{Metodologia}
W badaniu "Czy OpenAI Codex i inne duże modele językowe mogą pomóc nam naprawić błędy bezpieczeństwa?", autorzy skupili się na wykorzystaniu dużych modeli językowych (LLM) do naprawy podatności w kodzie w sposób zero-shot. Badanie koncentrowało się na projektowaniu monitów skłaniających LLM do generowania poprawionych wersji niebezpiecznego kodu. Przeprowadzono eksperymenty na szeroką skalę, obejmujące różne komercyjne modele LLM oraz lokalnie wytrenowany model.

\subsection{Wyniki}
Wyniki wykazały, że LLM mogą skutecznie naprawić 100\% syntetycznie wygenerowanych scenariuszy oraz 58\% podatności w historycznych błędach rzeczywistych projektów open-source. Odkryto, że różne sposoby formułowania informacji kluczowych w monitach wpływają na wyniki generowane przez modele. Zauważono, że wyższe temperatury generowania kodu przynoszą lepsze wyniki dla niektórych typów podatności, ale gorsze dla innych.

\section{Examining Zero-Shot Vulnerability Repair with Large Language Models}
% https://arxiv.org/pdf/2112.02125.pdf

W artykule "Examining Zero-Shot Vulnerability Repair with Large Language Models", autorzy kontynuowali badanie możliwości wykorzystania LLM do naprawy podatności w kodzie, koncentrując się na wyzwaniach związanych z generowaniem funkcjonalnie poprawnego kodu w rzeczywistych scenariuszach. Badanie to rozszerzało wcześniejsze prace, biorąc pod uwagę bardziej złożone przypadki użycia LLM.

Podstawowe pytania badawcze były następujące:
\begin{enumerate}
    \item Czy LLM mogą generować bezpieczny i funkcjonalny kod do naprawy podatności?
    \item Czy zmiana kontekstu w komentarzach wpływa na zdolność LLM do sugerowania poprawek?
    \item Jakie są wyzwania przy używaniu LLM do naprawy podatności w rzeczywistym świecie?
    \item Jak niezawodne są LLM w generowaniu napraw?
\end{enumerate}

Eksperymenty potwierdziły, że choć LLM wykazują potencjał, ich zdolność do generowania funkcjonalnych napraw w rzeczywistych warunkach jest ograniczona. Wyzwania związane z inżynierią promptów i ograniczenia modeli wskazują na potrzebę dalszych badań i rozwoju w tej dziedzinie.

\section{Różnice między obecną pracą a istniejącą literaturą}

W przeciwieństwie do dotychczasowych badań skoncentrowanych głównie na teoretycznym potencjale dużych modeli językowych (LLM) w kontekście zero-shot, niniejsza praca dyplomowa podejmuje kroki w kierunku praktycznego zastosowania tych technologii. Główną różnicą jest tutaj zastosowanie metod takich jak Retrieval Augmented Generation (RAG) oraz in-context learning, co przesuwa nasze podejście w stronę kontekstu few-shot. 

\begin{itemize}
    \item \textbf{Zastosowanie Metod RAG i In-context Learning:} W odróżnieniu od tradycyjnych podejść zero-shot, które polegają na generowaniu odpowiedzi bez uprzedniego dostosowania modelu do specyficznego zadania, moja praca wykorzystuje RAG i uczenie się w kontekście, aby lepiej dostosować modele do konkretnych scenariuszy związanych z bezpieczeństwem kodu. Te metody pozwalają na bardziej precyzyjną analizę i naprawę błędów w kodzie.
    
    \item \textbf{Praktyczne Zastosowanie Modeli Językowych:} Podczas gdy większość istniejących badań skupia się na badaniu możliwości SI w teorii, ta praca koncentruje się na praktycznym zastosowaniu modeli językowych do wykrywania i naprawiania błędów bezpieczeństwa w kodzie. Przez to podejście, praca ta dostarcza bezpośrednich, aplikatywnych rozwiązań, które mogą być wykorzystane w rzeczywistych środowiskach programistycznych.
\end{itemize}

Takie podejście pozwala nie tylko na zrozumienie teoretycznego potencjału LLM, ale także na ocenę ich praktycznej przydatności w realnych scenariuszach związanych z cyberbezpieczeństwem. Znacząco poszerza to zakres badań w dziedzinie wykorzystania sztucznej inteligencji do poprawy bezpieczeństwa aplikacji, dostarczając nowych perspektyw i rozwiązań.

\chapter{Metodyka rozwiązania}

W niniejszej pracy dyplomowej zastosowano szereg metod i środków, aby zaimplementować narzędzie do statycznej analizy kodu oraz zbadać i ocenić potencjał dużych modeli językowych w kontekście wykrywania i naprawiania błędów bezpieczeństwa w kodzie źródłowym aplikacji.

\begin{table}[H]
    \begin{adjustwidth}{-2cm}{-2cm}  % Zmniejszenie marginesów z obu stron
        \centering
    \begin{tabular}{|>{\bfseries}p{2.5cm}|p{5cm}|>{\bfseries}p{2.5cm}|p{5cm}|}
    \hline
    \multicolumn{4}{|c|}{\textbf{Metody i Środki}} \\
    \hline
    \textbf{Metoda} & \small{Opis} & \textbf{Środek} & \small{Opis} \\
    \hline
    \textbf{Zero-shot learning} & \small{Metoda uczenia maszynowego pozwalająca modelom wykonywać zadania bez wcześniejszego treningu, opierając się na zdolności do rozumienia i generalizacji.} & \textbf{Modele językowe GPT-3.5, GPT-4} & \small{Zaawansowane modele AI OpenAI do generowania tekstu i odpowiadania na zapytania.} \\
    \hline
    \textbf{Prompt engineering} & \small{Projektowanie promptów w celu uzyskania trafnych odpowiedzi od AI.} & \textbf{OpenAI Assistant API} & \small{API umożliwiające integrację modeli językowych w aplikacjach.} \\
    \hline
    \textbf{In-context learning} & \small{Uczenie się i dostosowywanie modeli AI na podstawie informacji z promptów.} & \textbf{Zbiory danych z kodem} & \small{Zestawy danych z przykładami kodu zawierającymi błędy, używane do trenowania narzędzi do wykrywania podatności.} \\
    \hline
    \textbf{Retrieval Augmented Generation} & \small{Technika łącząca generowanie treści z wyszukiwaniem informacji, wspomagana przez OpenAI Assistant API.} & \textbf{Projekty open-source zawierające błędy bezpieczeństwa} & \small{Publiczne projekty mogące zawierać błędy bezpieczeństwa, używane do edukacji.} \\
    \hline
    \textbf{Analiza porównawcza} & \small{Ocena różnych technik lub systemów poprzez porównanie.} & \textbf{Statyczne testy podatności} & \small{Narzędzia analizy statycznej kodu, np. CodeQL.} \\
    \hline
    \textbf{Programowanie obiektowe i funkcyjne} & \small{Dwa paradygmaty programowania, koncentrujące się odpowiednio na obiektach i funkcjach.} & \textbf{Rozwiązania komercyjne, np. Snyk} & \small{Narzędzia AI do zarządzania bezpieczeństwem oprogramowania.} \\
    \hline
    & & \textbf{Python 3.12} & \small{Najnowsza wersja języka Python z zaawansowanymi funkcjami.} \\
    \hline
    & & \textbf{Biblioteki: openai, asyncio} & \small{Biblioteki Pythona dla integracji z OpenAI i programowania asynchronicznego.} \\
    \hline
    & & \textbf{Komputer osobisty} & \small{Urządzenie do tworzenia i testowania oprogramowania.} \\
    \hline
    \end{tabular}
\end{adjustwidth}
    \caption{Metody i środki wykorzystane w projekcie i badaniu.}
    \label{tab:methods_tools}
\end{table}

    

Metody i środki te zostały wybrane, aby zapewnić efektywne i wszechstronne podejście do analizy i naprawy kodu. Generacja wspomagana pobieraniem danych (RAG ang. Retrieval Augmented Generation) 
oraz uczenie się w kontekście(in-context learning) umożliwiają efektywną analizę i generowanie kodu. 
Z kolei analiza porównawcza pozwala na ocenę skuteczności różnych modeli i podejść. 
Wykorzystanie modeli językowych GPT-3.5 i GPT-4, środowiska Ollama, oraz innych narzędzi i zasobów, zapewnia solidną bazę do przeprowadzenia kompleksowych testów i analiz.

