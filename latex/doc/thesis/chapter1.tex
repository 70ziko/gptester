
\addcontentsline{toc}{chapter}{Analiza istniejącej literatury oraz dotychczasowych badań}
\chapter{Analiza istniejącej literatury oraz dotychczasowych badań}

\section{Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs? - \scriptsize\textit{Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt}}
% https://arxiv.org/pdf/2112.02125v1.pdf

\subsection{Metodyka}
W badaniu "Czy OpenAI Codex i inne duże modele językowe mogą pomóc nam naprawić błędy bezpieczeństwa?"\cite{codex-fix-security-bugs} \url{https://arxiv.org/pdf/2112.02125v1.pdf} napisanym przez Hammond'a Pearce'a, Benjamina Tana, Baleegh'a Ahmad, Ramesh'a Karri oraz Brendan'a Dolan-Gavitt, autorzy skupili się na wykorzystaniu dużych modeli językowych (LLM) do naprawy podatności w kodzie w sposób zero-shot. Badanie koncentrowało się na projektowaniu monitów skłaniających LLM do generowania poprawionych wersji niebezpiecznego kodu. Przeprowadzono eksperymenty na szeroką skalę, obejmujące różne komercyjne modele LLM oraz lokalnie wytrenowany model.

\subsection{Wyniki}
Wyniki wykazały, że LLM mogą skutecznie naprawić 100\% syntetycznie wygenerowanych scenariuszy oraz 58\% podatności w historycznych błędach rzeczywistych projektów open-source. Odkryto, że różne sposoby formułowania informacji kluczowych w monitach wpływają na wyniki generowane przez modele. Zauważono, że wyższe temperatury generowania kodu przynoszą lepsze wyniki dla niektórych typów podatności, ale gorsze dla innych. 

Tak dobrych wyników niestety nie należy interpretować dosłownie, ponieważ z racji, że badanie przeprowadzono na reprezentatywnej próbie, autorzy nie byli w stanie ręcznie sprawdzać poprawności każdej naprawy i wykorzystali w tym celu istniejące narzędzia statycznej analizy kodu, takie jak CodeQL. W związku z powyższym, aby ocenić rzeczywistą skuteczność LLM w naprawianiu podatności, potrzebne są dalsze badania.
\section{Examining Zero-Shot Vulnerability Repair with Large Language Models - \scriptsize\textit{Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt}}
% https://arxiv.org/pdf/2112.02125.pdf


W pracy naukowej pt. "Examining Zero-Shot Vulnerability Repair with Large Language Models"\cite{zero-shot-vuln-repair} \url{https://arxiv.org/pdf/2112.02125.pdf}, autorzy przedłużają swoje badania nad potencjałem wykorzystania Large Language Models (LLM) w kontekście naprawy podatności w kodzie źródłowym. Niniejsze badanie koncentruje się na wyzwaniach związanych z generowaniem funkcjonalnie adekwatnego kodu w realistycznych warunkach aplikacyjnych. Rozszerzając zakres swoich wcześniejszych prac, autorzy skupiają się na bardziej skomplikowanych przypadkach użycia LLM, eksplorując ich zdolność do efektywnego i efektywnego adresowania złożonych problemów związanych z bezpieczeństwem oprogramowania.

Podstawowe pytania badawcze były następujące:
\begin{enumerate}
    \item Czy LLM mogą generować bezpieczny i funkcjonalny kod do naprawy podatności?
    \item Czy zmiana kontekstu w komentarzach wpływa na zdolność LLM do sugerowania poprawek?
    \item Jakie są wyzwania przy używaniu LLM do naprawy podatności w rzeczywistym świecie?
    \item Jak niezawodne są LLM w generowaniu napraw?
\end{enumerate}

Eksperymenty potwierdziły, że choć LLM wykazują potencjał, ich zdolność do generowania funkcjonalnych napraw w rzeczywistych warunkach jest ograniczona. Wyzwania związane z inżynierią promptów i ograniczenia modeli wskazują na potrzebę dalszych badań i rozwoju w tej dziedzinie.

\section{Różnice między obecną pracą a istniejącą literaturą}

W przeciwieństwie do dotychczasowych badań skoncentrowanych głównie na teoretycznym potencjale dużych modeli językowych (LLM) w kontekście zero-shot, niniejsza praca dyplomowa podejmuje kroki w kierunku praktycznego zastosowania tych technologii. Główną różnicą jest tutaj zastosowanie metod takich jak Retrieval Augmented Generation (RAG) oraz in-context learning, co przesuwa nasze podejście w stronę kontekstu few-shot. 

\begin{itemize}
    \item \textbf{Zastosowanie Metod RAG i In-context Learning:} W odróżnieniu od tradycyjnych podejść zero-shot, które polegają na generowaniu odpowiedzi bez uprzedniego dostosowania modelu do specyficznego zadania, moja praca wykorzystuje RAG i uczenie się w kontekście, aby lepiej dostosować modele do konkretnych scenariuszy związanych z bezpieczeństwem kodu. Te metody pozwalają na bardziej precyzyjną analizę i naprawę błędów w kodzie.
    
    \item \textbf{Praktyczne Zastosowanie Modeli Językowych:} Podczas gdy większość istniejących badań skupia się na badaniu możliwości SI w teorii, ta praca koncentruje się na praktycznym zastosowaniu modeli językowych do wykrywania i naprawiania błędów bezpieczeństwa w kodzie. Przez to podejście, praca ta dostarcza bezpośrednich, aplikatywnych rozwiązań, które mogą być wykorzystane w rzeczywistych środowiskach programistycznych.
\end{itemize}

Takie podejście pozwala nie tylko na zrozumienie teoretycznego potencjału LLM, ale także na ocenę ich praktycznej przydatności w realnych scenariuszach związanych z cyberbezpieczeństwem. Znacząco poszerza to zakres badań w dziedzinie wykorzystania sztucznej inteligencji do poprawy bezpieczeństwa aplikacji, dostarczając nowych perspektyw i rozwiązań.

\chapter{Metodyka rozwiązania}

W niniejszej pracy dyplomowej zastosowano szereg metod i środków, aby zaimplementować narzędzie do statycznej analizy kodu oraz zbadać i ocenić potencjał dużych modeli językowych w kontekście wykrywania i naprawiania błędów bezpieczeństwa w kodzie źródłowym aplikacji.

\begin{table}[H]
    \begin{adjustwidth}{-2cm}{-2cm}  % Zmniejszenie marginesów z obu stron
        \centering
    \begin{tabular}{|>{\bfseries}p{2.7cm}|p{5cm}|>{\bfseries}p{2.5cm}|p{5cm}|}
    \hline
    \multicolumn{4}{|c|}{\textbf{Metody i Środki}} \\
    \hline
    \textbf{Metoda} & \small{Opis} & \textbf{Środek} & \small{Opis} \\
    \hline
    \textbf{Zero-shot learning} & \small{Metoda uczenia maszynowego pozwalająca modelom wykonywać zadania bez wcześniejszego treningu, opierając się na zdolności do rozumienia i generalizacji.} & \textbf{Modele językowe GPT-3.5, GPT-4} & \small{Zaawansowane modele AI OpenAI do generowania tekstu i odpowiadania na zapytania.} \\
    \hline
    \textbf{Prompt engineering} & \small{Projektowanie promptów w celu uzyskania trafnych odpowiedzi od AI.} & \textbf{OpenAI Assistant API} & \small{API umożliwiające integrację modeli językowych w aplikacjach.} \\
    \hline
    \textbf{In-context learning} & \small{Uczenie się i dostosowywanie modeli AI na podstawie informacji zawartych w kontekście zapytań.} & \textbf{Zbiory danych z kodem} & \small{Zestawy danych z przykładami kodu zawierającymi błędy, używane do trenowania narzędzi do wykrywania podatności.} \\
    \hline
    \textbf{Retrieval Augmented Generation} & \small{Technika łącząca generowanie treści z wyszukiwaniem informacji, wspomagana przez OpenAI Assistant API.} & \textbf{Projekty open-source zawierające podatności} & \small{Publiczne projekty zawierające błędy bezpieczeństwa, używane w testowaniu aplikacji oraz ocenie skuteczności LLM.} \\
    \hline
    \textbf{Analiza porównawcza} & \small{Ocena różnych technik lub systemów poprzez porównanie.} & \textbf{Statyczne testy podatności} & \small{Narzędzia analizy statycznej kodu, np. CodeQL.} \\
    \hline
    \textbf{Programowanie obiektowe i funkcyjne} & \small{Dwa paradygmaty programowania, koncentrujące się odpowiednio na obiektach i funkcjach.} & \textbf{Rozwiązania komercyjne, np. Snyk} & \small{Narzędzia AI do zarządzania bezpieczeństwem oprogramowania.} \\
    \hline
    & & \textbf{Python 3.12} & \small{Najnowsza wersja języka Python z zaawansowanymi funkcjami.} \\
    \hline
    & & \textbf{Biblioteki: openai, asyncio} & \small{Biblioteki Pythona dla integracji z OpenAI i programowania asynchronicznego.} \\
    \hline
    & & \textbf{Komputer osobisty} & \small{Urządzenie do tworzenia i testowania oprogramowania.} \\
    \hline
    \end{tabular}
\end{adjustwidth}
    \caption{Metody i środki wykorzystane w projekcie i badaniu.}
    \label{tab:methods_tools}
\end{table}

    

Metody i środki te zostały wybrane, aby zapewnić efektywne i wszechstronne podejście do analizy i naprawy kodu. Generacja wspomagana pobieraniem danych (RAG ang. Retrieval Augmented Generation) 
oraz uczenie się w kontekście(in-context learning) umożliwiają efektywną analizę i generowanie kodu. 
Z kolei analiza porównawcza pozwala na ocenę skuteczności różnych modeli i podejść. 
Wykorzystanie modeli językowych GPT-3.5 i GPT-4, środowiska Ollama, oraz innych narzędzi i zasobów, zapewnia solidną bazę do przeprowadzenia kompleksowych testów i analiz.

